{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "https://www.geeksforgeeks.org/pandas/handling-large-datasets-in-pandas/",
   "id": "e7e2fce99d53ceba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Handling Large Datasets in Pandas\n",
    "Last Updated : 23 Jul, 2025\n",
    "Pandas is an excellent tool for working with smaller datasets, typically ranging from two to three gigabytes. However, when the dataset size exceeds this threshold, using Pandas can become problematic. This is because Pandas loads the entire dataset into memory before processing it, which can cause memory issues if the dataset is too large for the available RAM. Even with smaller datasets, memory problems can arise as preprocessing and modifications often create duplicate copies of the DataFrame.\n",
    "\n",
    "Despite these challenges, there are several techniques that allow you to handle larger datasets efficiently with Pandas in Python. Letâ€™s explore these methods that enable you to work with millions of records while minimizing memory usage.\n",
    "\n",
    "# How to handle Large Datasets in Python?\n",
    "1. Use Efficient Datatypes: Utilize more memory-efficient data types (e.g., int32 instead of int64, float32 instead of float64) to reduce memory usage.\n",
    "2. Load Less Data: Use the use-cols parameter in pd.read_csv() to load only the necessary columns, reducing memory consumption.\n",
    "3. Sampling: For exploratory data analysis or testing, consider working with a sample of the dataset instead of the entire dataset.\n",
    "4. Chunking: Use the chunksize parameter in pd.read_csv() to read the dataset in smaller chunks, processing each chunk iteratively.\n",
    "5. Optimizing Pandas dtypes: Use the astype method to convert columns to more memory-efficient types after loading the data, if appropriate.\n",
    "6. Parallelizing Pandas with Dask: Use Dask, a parallel computing library, to scale Pandas workflows to larger-than-memory datasets by leveraging parallel processing.\n",
    "\n",
    "Using Efficient Data Types:\n",
    "\n",
    "* Reducing memory utilization in Pandas requires the use of efficient data types. For instance, if precision allows, you can use float32 or even float16 in instead of the standard float64 dtype. Similar to this, if the data range permits, integer columns can be downcast to smaller integer types like int8, int16, or int32.\n",
    "* Benefits: Significantly lessens memory footprint, particularly for big datasets.\n",
    "* Implementation: When reading data, you can use functions like pd.read_csv() or pd.read_sql() to specify the dtype parameter. Furthermore, existing columns can be changed to more memory-efficient types using the astype() method."
   ],
   "id": "efe62cc7704ffaf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.147942Z",
     "start_time": "2025-12-15T00:14:42.158564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the size of the dataset\n",
    "num_rows = 1000000  # 1 million rows\n",
    "\n",
    "# Example DataFrame with inefficient datatypes\n",
    "data = {'A': [1, 2, 3, 4],\n",
    "        'B': [5.0, 6.0, 7.0, 8.0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Replicate the DataFrame to create a larger dataset\n",
    "df_large = pd.concat([df] * (num_rows // len(df)), ignore_index=True)\n",
    "\n",
    "# Check memory usage before conversion\n",
    "print(\"Memory usage before conversion:\")\n",
    "print(df_large.memory_usage(deep=True).sum())\n",
    "\n",
    "# Convert to more memory-efficient datatypes\n",
    "df_large['A'] = pd.to_numeric(df_large['A'], downcast='integer')\n",
    "df_large['B'] = pd.to_numeric(df_large['B'], downcast='float')\n",
    "\n",
    "# Check memory usage after conversion\n",
    "print(\"Memory usage after conversion:\")\n",
    "print(df_large.memory_usage(deep=True).sum())"
   ],
   "id": "5bba4b12102be4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before conversion:\n",
      "16000132\n",
      "Memory usage after conversion:\n",
      "5000132\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load Less Data\n",
    "\n",
    "* Overview: This technique entails loading only the relevant columns from the dataset. This is especially helpful when working with datasets that have a lot of columns or when analysis just requires a portion of the data.\n",
    "* Benefits: Enhances processing effectiveness and uses less memory.\n",
    "* Implementation: To select which columns to load, use the usecols parameter in routines such as pd.read_csv()."
   ],
   "id": "1cf0902c85ea575a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.162949Z",
     "start_time": "2025-12-15T00:14:44.148670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'A': range(1000),\n",
    "        'B': range(1000),\n",
    "        'C': range(1000),\n",
    "        'D': range(1000)}\n",
    "\n",
    "# Load only specific columns\n",
    "df = pd.DataFrame(data)\n",
    "df_subset = df[['A', 'D']]\n",
    "print('Specific Columns of the DataFrame')\n",
    "print(df_subset.head())"
   ],
   "id": "5e46393583e7a1e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific Columns of the DataFrame\n",
      "   A  D\n",
      "0  0  0\n",
      "1  1  1\n",
      "2  2  2\n",
      "3  3  3\n",
      "4  4  4\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sampling:\n",
    "\n",
    "* Sampling is the process of choosing a random selection of the dataset's data for examination. This can be used to quickly analyze the dataset, explore it, or create models using a representative sample of the data.\n",
    "* Benefits: Makes analysis and experimentation faster, especially when working with big datasets.\n",
    "* Implementation: To randomly select rows or columns from the DataFrame, use Pandas' sample() method."
   ],
   "id": "9c3c73879ab446ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.183145Z",
     "start_time": "2025-12-15T00:14:44.164660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'A': range(1000),\n",
    "        'B': range(1000),\n",
    "        'C': range(1000),\n",
    "        'D': range(1000)}\n",
    "\n",
    "# Sample 10% of the dataset\n",
    "df = pd.DataFrame(data)\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "print(df_sample.head())"
   ],
   "id": "b1a03f6f548d1234",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A    B    C    D\n",
      "521  521  521  521  521\n",
      "737  737  737  737  737\n",
      "740  740  740  740  740\n",
      "660  660  660  660  660\n",
      "411  411  411  411  411\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chunking:\n",
    "\n",
    "* Rather than loading the complete dataset into memory at once, chunking entails processing the dataset in smaller, more manageable parts. When working with datasets that are too big to fit in memory, this is quite helpful.\n",
    "* Benefits: Processes huge datasets on devices with limited memory and uses less memory.\n",
    "* Implementation: To specify the number of rows to read at a time, use the chunksize argument in routines such as pd.read_csv()."
   ],
   "id": "b13667e6bf175dec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.221009Z",
     "start_time": "2025-12-15T00:14:44.191208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'A': range(10000),\n",
    "        'B': range(10000)}\n",
    "\n",
    "# Process data in chunks\n",
    "chunk_size = 1000\n",
    "for chunk in pd.DataFrame(data).groupby(pd.DataFrame(data).index // chunk_size):\n",
    "    print(chunk)"
   ],
   "id": "ca73385a191e4f10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,        A    B\n",
      "0      0    0\n",
      "1      1    1\n",
      "2      2    2\n",
      "3      3    3\n",
      "4      4    4\n",
      "..   ...  ...\n",
      "995  995  995\n",
      "996  996  996\n",
      "997  997  997\n",
      "998  998  998\n",
      "999  999  999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(1,          A     B\n",
      "1000  1000  1000\n",
      "1001  1001  1001\n",
      "1002  1002  1002\n",
      "1003  1003  1003\n",
      "1004  1004  1004\n",
      "...    ...   ...\n",
      "1995  1995  1995\n",
      "1996  1996  1996\n",
      "1997  1997  1997\n",
      "1998  1998  1998\n",
      "1999  1999  1999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(2,          A     B\n",
      "2000  2000  2000\n",
      "2001  2001  2001\n",
      "2002  2002  2002\n",
      "2003  2003  2003\n",
      "2004  2004  2004\n",
      "...    ...   ...\n",
      "2995  2995  2995\n",
      "2996  2996  2996\n",
      "2997  2997  2997\n",
      "2998  2998  2998\n",
      "2999  2999  2999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(3,          A     B\n",
      "3000  3000  3000\n",
      "3001  3001  3001\n",
      "3002  3002  3002\n",
      "3003  3003  3003\n",
      "3004  3004  3004\n",
      "...    ...   ...\n",
      "3995  3995  3995\n",
      "3996  3996  3996\n",
      "3997  3997  3997\n",
      "3998  3998  3998\n",
      "3999  3999  3999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(4,          A     B\n",
      "4000  4000  4000\n",
      "4001  4001  4001\n",
      "4002  4002  4002\n",
      "4003  4003  4003\n",
      "4004  4004  4004\n",
      "...    ...   ...\n",
      "4995  4995  4995\n",
      "4996  4996  4996\n",
      "4997  4997  4997\n",
      "4998  4998  4998\n",
      "4999  4999  4999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(5,          A     B\n",
      "5000  5000  5000\n",
      "5001  5001  5001\n",
      "5002  5002  5002\n",
      "5003  5003  5003\n",
      "5004  5004  5004\n",
      "...    ...   ...\n",
      "5995  5995  5995\n",
      "5996  5996  5996\n",
      "5997  5997  5997\n",
      "5998  5998  5998\n",
      "5999  5999  5999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(6,          A     B\n",
      "6000  6000  6000\n",
      "6001  6001  6001\n",
      "6002  6002  6002\n",
      "6003  6003  6003\n",
      "6004  6004  6004\n",
      "...    ...   ...\n",
      "6995  6995  6995\n",
      "6996  6996  6996\n",
      "6997  6997  6997\n",
      "6998  6998  6998\n",
      "6999  6999  6999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(7,          A     B\n",
      "7000  7000  7000\n",
      "7001  7001  7001\n",
      "7002  7002  7002\n",
      "7003  7003  7003\n",
      "7004  7004  7004\n",
      "...    ...   ...\n",
      "7995  7995  7995\n",
      "7996  7996  7996\n",
      "7997  7997  7997\n",
      "7998  7998  7998\n",
      "7999  7999  7999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(8,          A     B\n",
      "8000  8000  8000\n",
      "8001  8001  8001\n",
      "8002  8002  8002\n",
      "8003  8003  8003\n",
      "8004  8004  8004\n",
      "...    ...   ...\n",
      "8995  8995  8995\n",
      "8996  8996  8996\n",
      "8997  8997  8997\n",
      "8998  8998  8998\n",
      "8999  8999  8999\n",
      "\n",
      "[1000 rows x 2 columns])\n",
      "(9,          A     B\n",
      "9000  9000  9000\n",
      "9001  9001  9001\n",
      "9002  9002  9002\n",
      "9003  9003  9003\n",
      "9004  9004  9004\n",
      "...    ...   ...\n",
      "9995  9995  9995\n",
      "9996  9996  9996\n",
      "9997  9997  9997\n",
      "9998  9998  9998\n",
      "9999  9999  9999\n",
      "\n",
      "[1000 rows x 2 columns])\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optimising Pandas dtypes:\n",
    "\n",
    "* Described as: Finding columns with data types that are not as efficient as possible and changing them to ones that are would save more memory. Performance can be greatly enhanced and memory utilization can be much decreased.\n",
    "* Benefits: Increases processing speed and minimizes memory footprint.\n",
    "* Implementation: To convert columns to more efficient data types, use the astype() method. To convert columns to datetime or numeric types, respectively, use functions such as pd.to_datetime() or pd.to_numeric()."
   ],
   "id": "a547bbf2591a2781"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.239245Z",
     "start_time": "2025-12-15T00:14:44.222232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'date_column': ['2022-01-01', '2022-01-02', '2022-01-03'],\n",
    "        'numeric_column': [1.234, 2.345, 3.456]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert inefficient dtypes\n",
    "df['date_column'] = pd.to_datetime(df['date_column'])\n",
    "df['numeric_column'] = pd.to_numeric(df['numeric_column'], downcast='float')\n",
    "\n",
    "print(df.dtypes)"
   ],
   "id": "d8a790c2521788e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_column       datetime64[ns]\n",
      "numeric_column           float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parallelising Pandas with Dask:\n",
    "\n",
    "* Dask is a package for parallel computing that works well with Pandas and offers parallelized operations for big datasets. Your Pandas workflows can be scaled across many cores or even distributed clusters with its help.\n",
    "* Advantages: Allows Pandas operations to be executed in parallel, greatly reducing processing times for huge datasets.\n",
    "* Implementation: To execute parallelized operations on sizable datasets, use Dask data structures like dask.DataFrame and dask.array. Dask facilitates the smooth transfer of current codebases to parallel execution by supporting the majority of the well-known Pandas APIs."
   ],
   "id": "25e0af9e7b0ee579"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.263101Z",
     "start_time": "2025-12-15T00:14:44.240883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ],
   "id": "4fd99540685261f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.281068Z",
     "start_time": "2025-12-15T00:14:44.263570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd, dask\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"dask version:\", dask.__version__)\n",
    "print(\"string storage mode:\", pd.options.mode.string_storage)"
   ],
   "id": "8b2a5a330352917b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.3.3\n",
      "dask version: 2025.12.0\n",
      "string storage mode: python\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.287050Z",
     "start_time": "2025-12-15T00:14:44.282633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.string_storage = \"python\"  # <-- key line"
   ],
   "id": "ad9f62b7294f0813",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.304987Z",
     "start_time": "2025-12-15T00:14:44.287457Z"
    }
   },
   "cell_type": "code",
   "source": "print(pd.options.mode.string_storage)  # should print \"python\"",
   "id": "430594c05722ce5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'A': range(10000),\n",
    "        'B': range(10000)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Load data using Dask\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "# Perform parallelized operations\n",
    "result = ddf.groupby('A').mean().compute()\n",
    "print(result)"
   ],
   "id": "1f4cf06d617ce58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:14:44.392302Z",
     "start_time": "2025-12-15T00:14:44.306484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.string_storage = \"python\"   # <-- critical line\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'A': range(10000),\n",
    "        'B': range(10000)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Load data using Dask\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "# Perform parallelized operations\n",
    "result = ddf.groupby('A').mean().compute()\n",
    "print(result)"
   ],
   "id": "6d4a43616d08e2e9",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "pyarrow>=10.0.1 is required for PyArrow backed StringArray.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[94]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      9\u001B[39m df = pd.DataFrame(data)\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# Load data using Dask\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m ddf = \u001B[43mdd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnpartitions\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Perform parallelized operations\u001B[39;00m\n\u001B[32m     15\u001B[39m result = ddf.groupby(\u001B[33m'\u001B[39m\u001B[33mA\u001B[39m\u001B[33m'\u001B[39m).mean().compute()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/PythonProject/PythonProject54/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_collection.py:4918\u001B[39m, in \u001B[36mfrom_pandas\u001B[39m\u001B[34m(data, npartitions, sort, chunksize)\u001B[39m\n\u001B[32m   4912\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m   4913\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPlease provide chunksize as an int, or possibly as None if you specify npartitions.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4914\u001B[39m     )\n\u001B[32m   4916\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdask\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdataframe\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdask_expr\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mio\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mio\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FromPandas\n\u001B[32m-> \u001B[39m\u001B[32m4918\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnew_collection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4919\u001B[39m \u001B[43m    \u001B[49m\u001B[43mFromPandas\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4920\u001B[39m \u001B[43m        \u001B[49m\u001B[43m_BackendData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4921\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnpartitions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnpartitions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4922\u001B[39m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4923\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4924\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpyarrow_strings_enabled\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpyarrow_strings_enabled\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4925\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4926\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/PythonProject/PythonProject54/.venv/lib/python3.13/site-packages/dask/_collections.py:8\u001B[39m, in \u001B[36mnew_collection\u001B[39m\u001B[34m(expr)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mnew_collection\u001B[39m(expr):\n\u001B[32m      7\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Create new collection from an expr\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     meta = \u001B[43mexpr\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_meta\u001B[49m\n\u001B[32m      9\u001B[39m     expr._name  \u001B[38;5;66;03m# Ensure backend is imported\u001B[39;00m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m get_collection_type(meta)(expr)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:1026\u001B[39m, in \u001B[36mcached_property.__get__\u001B[39m\u001B[34m(self, instance, owner)\u001B[39m\n\u001B[32m   1024\u001B[39m val = cache.get(\u001B[38;5;28mself\u001B[39m.attrname, _NOT_FOUND)\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m     val = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1027\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1028\u001B[39m         cache[\u001B[38;5;28mself\u001B[39m.attrname] = val\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/PythonProject/PythonProject54/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/io/io.py:445\u001B[39m, in \u001B[36mFromPandas._meta\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    442\u001B[39m \u001B[38;5;129m@functools\u001B[39m.cached_property\n\u001B[32m    443\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_meta\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    444\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pyarrow_strings_enabled:\n\u001B[32m--> \u001B[39m\u001B[32m445\u001B[39m         meta = make_meta(\u001B[43mto_pyarrow_string\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    446\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    447\u001B[39m         meta = \u001B[38;5;28mself\u001B[39m.frame.head(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/PythonProject/PythonProject54/.venv/lib/python3.13/site-packages/dask/dataframe/_pyarrow.py:61\u001B[39m, in \u001B[36m_to_string_dtype\u001B[39m\u001B[34m(df, dtype_check, index_check, string_dtype)\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[38;5;66;03m# Guards against importing `pyarrow` at the module level (where it may not be installed)\u001B[39;00m\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m string_dtype == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m     string_dtype = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mStringDtype\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpyarrow\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[38;5;66;03m# Possibly convert DataFrame/Series/Index to `string[pyarrow]`\u001B[39;00m\n\u001B[32m     64\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_dataframe_like(df):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/PythonProject/PythonProject54/.venv/lib/python3.13/site-packages/pandas/core/arrays/string_.py:182\u001B[39m, in \u001B[36mStringDtype.__init__\u001B[39m\u001B[34m(self, storage, na_value)\u001B[39m\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    179\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mStorage must be \u001B[39m\u001B[33m'\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m'\u001B[39m\u001B[33m or \u001B[39m\u001B[33m'\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m'\u001B[39m\u001B[33m. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstorage\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    180\u001B[39m     )\n\u001B[32m    181\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m storage == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m pa_version_under10p1:\n\u001B[32m--> \u001B[39m\u001B[32m182\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m    183\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mpyarrow>=10.0.1 is required for PyArrow backed StringArray.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    184\u001B[39m     )\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(na_value, \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m np.isnan(na_value):\n\u001B[32m    187\u001B[39m     \u001B[38;5;66;03m# when passed a NaN value, always set to np.nan to ensure we use\u001B[39;00m\n\u001B[32m    188\u001B[39m     \u001B[38;5;66;03m# a consistent NaN value (and we can use `dtype.na_value is np.nan`)\u001B[39;00m\n\u001B[32m    189\u001B[39m     na_value = np.nan\n",
      "\u001B[31mImportError\u001B[39m: pyarrow>=10.0.1 is required for PyArrow backed StringArray."
     ]
    }
   ],
   "execution_count": 94
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
